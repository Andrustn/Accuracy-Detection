{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93685da9-c0fe-4f17-b339-5c2faaf6ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import Tree\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy import tokenizer\n",
    "from tqdm import tqdm \n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re\n",
    "import spacy \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4380d6-fd04-4002-84a2-d7d37799be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text pulled from ChatGPT\n",
    "text = \"\"\"The President of the United States in 2008 was George W. Bush. \n",
    "He served as the 43rd President of the United States from 2001 to 2009.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b0b1d7-b5d4-482e-bf68-19e147501a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The President of the United States in 2008 was George W. Bush.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split example text into sentences, print the first sentence\n",
    "sentence = sent_tokenize(text)[0]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b59a10-cd9c-4b62-a180-5a62708ccac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('President', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('United', 'NNP'),\n",
       " ('States', 'NNPS'),\n",
       " ('in', 'IN'),\n",
       " ('2008', 'CD'),\n",
       " ('was', 'VBD'),\n",
       " ('George', 'NNP'),\n",
       " ('W.', 'NNP'),\n",
       " ('Bush', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Report parts of speech\n",
    "tagged_text = word_tokenize(sentence)\n",
    "nltk.pos_tag(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4bea729-4a5c-43dc-9dcf-0baee1d6707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in 2008 was \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    George W. Bush\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# named entity recognition\n",
    "doc = nlp(sentence)\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09dda54-0cc9-4dd9-8a70-5439547c4d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> det --> DET\n",
      "President --> nsubj --> PROPN\n",
      "of --> prep --> ADP\n",
      "the --> det --> DET\n",
      "United --> compound --> PROPN\n",
      "States --> pobj --> PROPN\n",
      "in --> prep --> ADP\n",
      "2008 --> pobj --> NUM\n",
      "was --> ROOT --> AUX\n",
      "George --> compound --> PROPN\n",
      "W. --> compound --> PROPN\n",
      "Bush --> attr --> PROPN\n",
      ". --> punct --> PUNCT\n"
     ]
    }
   ],
   "source": [
    "# combining depedencies and parts of speech\n",
    "for tok in doc: \n",
    "    print(tok.text, \"-->\",tok.dep_,\"-->\", tok.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31caa247-f132-4672-8787-691c4bde339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          was                       \n",
      "  _________________________|_________________        \n",
      " |      President                            |      \n",
      " |    ______|____________________            |       \n",
      " |   |              of           |           |      \n",
      " |   |              |            |           |       \n",
      " |   |            States         in         Bush    \n",
      " |   |       _______|______      |      _____|____   \n",
      " .  The    the           United 2008 George       W.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing dependency tree\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5363e3b-02e1-4e84-96c7-8582623c5050",
   "metadata": {},
   "source": [
    "### The below features alternative methods that were expirimented with, and sandbox code to impliment dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84adbda6-de83-47b7-8258-69f9e740fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]\n",
    "ADJECTIVES = [\"acomp\", \"advcl\", \"advmod\", \"amod\", \"appos\", \"nn\", \"nmod\", \"ccomp\", \"complm\",\n",
    "              \"hmod\", \"infmod\", \"xcomp\", \"rcmod\", \"poss\",\" possessive\"]\n",
    "COMPOUNDS = [\"compound\"]\n",
    "PREPOSITIONS = [\"prep\"]\n",
    "\n",
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs\n",
    "\n",
    "def getAdjectives(toks):\n",
    "    toks_with_adjectives = []\n",
    "    for tok in toks:\n",
    "        adjs = [left for left in tok.lefts if left.dep_ in ADJECTIVES]\n",
    "        adjs.append(tok)\n",
    "        adjs.extend([right for right in tok.rights if tok.dep_ in ADJECTIVES])\n",
    "        tok_with_adj = \" \".join([adj.lower_ for adj in adjs])\n",
    "        toks_with_adjectives.extend(adjs)\n",
    "\n",
    "    return toks_with_adjectives\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def getAllObjsWithAdjectives(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "\n",
    "    if len(objs)== 0:\n",
    "        objs = [tok for tok in rights if tok.dep_ in ADJECTIVES]\n",
    "\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos\n",
    "\n",
    "def findSVAOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjsWithAdjectives(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    obj_desc_tokens = generate_left_right_adjectives(obj)\n",
    "                    sub_compound = generate_sub_compound(sub)\n",
    "                    svos.append((\" \".join(tok.lower_ for tok in sub_compound), \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, \" \".join(tok.lower_ for tok in obj_desc_tokens)))\n",
    "    return svos\n",
    "\n",
    "def generate_sub_compound(sub):\n",
    "    sub_compunds = []\n",
    "    for tok in sub.lefts:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    sub_compunds.append(sub)\n",
    "    for tok in sub.rights:\n",
    "        if tok.dep_ in COMPOUNDS:\n",
    "            sub_compunds.extend(generate_sub_compound(tok))\n",
    "    return sub_compunds\n",
    "\n",
    "def generate_left_right_adjectives(obj):\n",
    "    obj_desc_tokens = []\n",
    "    for tok in obj.lefts:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "    obj_desc_tokens.append(obj)\n",
    "\n",
    "    for tok in obj.rights:\n",
    "        if tok.dep_ in ADJECTIVES:\n",
    "            obj_desc_tokens.extend(generate_left_right_adjectives(tok))\n",
    "\n",
    "    return obj_desc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b6e7156-eda4-4b2c-9df9-fb97db1bb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(u\"The Empire of Japan aimed to dominate Asia and the \" \\\n",
    "               \"Pacific and was already at war with the Republic of China \" \\\n",
    "               \"in 1937, but the world war is generally said to have begun on \" \\\n",
    "               \"1 September 1939 with the invasion of Poland by Germany and \" \\\n",
    "               \"subsequent declarations of war on Germany by France and the United Kingdom. \" \\\n",
    "               \"From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered \" \\\n",
    "               \"or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan. \" \\\n",
    "               \"Under the Molotov-Ribbentrop Pact of August 1939, Germany and the Soviet Union partitioned and \" \\\n",
    "               \"annexed territories of their European neighbours, Poland, Finland, Romania and the Baltic states. \" \\\n",
    "               \"The war continued primarily between the European Axis powers and the coalition of the United Kingdom \" \\\n",
    "               \"and the British Commonwealth, with campaigns including the North Africa and East Africa campaigns, \" \\\n",
    "               \"the aerial Battle of Britain, the Blitz bombing campaign, the Balkan Campaign as well as the \" \\\n",
    "               \"long-running Battle of the Atlantic. In June 1941, the European Axis powers launched an invasion \" \\\n",
    "               \"of the Soviet Union, opening the largest land theatre of war in history, which trapped the major part \" \\\n",
    "               \"of the Axis' military forces into a war of attrition. In December 1941, Japan attacked \" \\\n",
    "               \"the United States and European territories in the Pacific Ocean, and quickly conquered much of \" \\\n",
    "               \"the Western Pacific.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "945e2e84-6a8f-419f-820f-62d733647b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([Germany], [conquered], [Europe]),\n",
       " ([Japan], [attacked], [the, United, States])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipes_structure = [SequencePipe([FindTokensPipe(\"VERB/nsubj/*\"),\n",
    "                                 NamedEntityFilterPipe(),\n",
    "                                 NamedEntityExtractorPipe()]),\n",
    "                   FindTokensPipe(\"VERB\"),\n",
    "                   AnyPipe([SequencePipe([FindTokensPipe(\"VBD/dobj/NNP\"),\n",
    "                                          AggregatePipe([NamedEntityFilterPipe(\"GPE\"), \n",
    "                                                NamedEntityFilterPipe(\"PERSON\")]),\n",
    "                                          NamedEntityExtractorPipe()]),\n",
    "                            SequencePipe([FindTokensPipe(\"VBD/**/*/pobj/NNP\"),\n",
    "                                          AggregatePipe([NamedEntityFilterPipe(\"LOC\"), \n",
    "                                                NamedEntityFilterPipe(\"PERSON\")]),\n",
    "                                          NamedEntityExtractorPipe()])])]\n",
    "\n",
    "engine = PipelineEngine(pipes_structure, Context(doc), [0,1,2])\n",
    "engine.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ab287a-5c11-4a03-98aa-be23fcb0f313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['President', 'United 2008']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "068665d4-4e2b-4f3a-8d1f-672f9bd72741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities('the dog is brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba107e6a-c8f2-43cc-80ed-0eef958dbcb3",
   "metadata": {},
   "source": [
    "https://hami-asmai.medium.com/relationship-extraction-from-any-web-articles-using-spacy-and-jupyter-notebook-in-6-steps-4444ee68763f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a488b-84bc-4347-b8b1-947469ca19e5",
   "metadata": {},
   "source": [
    "types of subject object relations\n",
    "https://suttipong-kull.medium.com/how-to-extract-subject-verb-and-object-by-nlp-4149323a7d7d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75a0c5-bd1c-47f6-a6e6-df3d12cdcf03",
   "metadata": {},
   "source": [
    "# knowledge graph creation\n",
    "https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef358a-ce37-42ba-9031-ec1ee4a5cdf0",
   "metadata": {},
   "source": [
    "# pipelines to identify specific relationships?\n",
    "https://stackoverflow.com/questions/39763091/how-to-extract-subjects-in-a-sentence-and-their-respective-dependent-phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c17c05-bd0e-4e34-bb5d-10d6d476b3b9",
   "metadata": {},
   "source": [
    "# information extraction\n",
    "https://www.analyticsvidhya.com/blog/2019/09/introduction-information-extraction-python-spacy/?utm_source=blog&utm_medium=how-to-build-knowledge-graph-text-using-spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
